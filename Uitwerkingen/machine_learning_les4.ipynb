{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc90d42f",
   "metadata": {},
   "source": [
    "# Machine Learning - Les 4: Unsupervised learning\n",
    "\n",
    "Notebook bij les 4 van de leerlijn machine learning van S3 - AI. \n",
    "\n",
    "© Auteur: Rianne van Os\n",
    "\n",
    "**Voorbereiding voor deze les:**\n",
    "- Bekijk onderstaande theorie en maak t/m Opdracht 4.1.\n",
    "- Bekijk deze YouTube video: https://youtu.be/R2e3Ls9H_fc\n",
    "\n",
    "\n",
    "In de vorige 3 machine learning lessen ben je aan de slag gegaan met *supervised learning*. Dit is de term voor het maken van modellen waarbij je een *target* variabele gaat voorspellen. Belangrijk daarbij is dan ook dat je historische data hebt waarbij die target al bekend is.\n",
    "\n",
    "Maar ook als er geen targetvariabele is, zijn er machine learning technieken die je in kunt zetten om informatie uit je data te halen. We spreken dan van *unsupervised learning*. Er zijn geen labels bekend, we gaan dus niks 'voorspellen', maar we gaan zoeken naar patronen in de data die ons informatie geven. \n",
    "\n",
    "Hieronder enkele toepassingen:\n",
    "- **Klantsegmentatie**  \n",
    "Een streamingsdienst heeft miljoenen kijkers, maar deze klanten hebben geen specifieke labels zoals 'actiefilmfan' of 'comedyliefhebber'. Met clustering kun je deze kijkers toch groeperen. Je traint een algoritme dat  kijkgedrag analyseert (welke films kijken ze, hoe lang, etc.) en clusters vormt van mensen met vergelijkbare voorkeuren. Als iemand in een cluster een bepaalde film leuk vindt, kan die film aan vergelijkbare klanten in dezelfde groep worden aanbevolen. \n",
    "- **Opsporen van outliers**  \n",
    "Clustering kan ook helpen bij het vinden van **outliers**. Datapunten die niet in een logisch cluster passen zijn mogelijke outliers, hiermee kun je je dataset opschonen en je supervised modellen verbeteren. (Denk aan de outliers die we gezien hebben in een scatterplot met lengte en gewicht van personen.)\n",
    "- **Associatieregels**  \n",
    "Een supermarkt kan clustering gebruiken om te bepalen welke producten klanten vaak samen kopen. Het algoritme analyseert duizenden aankooptransacties en groepeert producten die vaak in dezelfde winkelwagen belanden. Hierdoor ontstaan patronen zoals 'mensen die luiers kopen, kopen ook vaak bier'. Met deze kennis kan de supermarkt productplaatsing optimaliseren of gerichte aanbiedingen doen.\n",
    "- **Anomaly detection**  \n",
    "Bij fraudebestrijding of het detecteren van cyberaanvallen weet je niet altijd hoe 'fraude' of 'aanval' er precies uitziet (je hebt geen gelabelde voorbeelden van alle mogelijke fraudegevallen). *Anomaly detection* is een vorm van unsupervised learning waarbij het algoritme leert wat normaal gedrag is en alle data die niet binnen een normaal patroon vallen kunnen aangemerkt worden als _anomalie_ (iets abnormaals, dus mogelijk fraude of een aanval). Overigens kunnen er ook supervised learning technieken gebruikt worden voor anomaliedetectie.\n",
    "- **Dimensionality reduction**  \n",
    "Soms werk je met datasets die zoveel variabelen (features) hebben dat ze onhandelbaar worden – honderden of zelfs duizenden. Dit maakt visualisatie onmogelijk en vertraagt supervised modellen. Dimensionaliteitsreductie is een unsupervised techniek die hierbij helpt. Technieken zoals Principal Component Analysis (PCA) vinden de belangrijkste onderliggende patronen in deze hoog-dimensionale data en projecteren de data op een lager aantal dimensies, zonder dat je vooraf 'belangrijke' variabelen hoeft te labelen.\n",
    "\n",
    "In dit college gaan we aan de slag met clustering, met name komen de algoritmes _k_-Means en DBSCAN aan bod. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b4edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importeer benodigde libraries - voeg toe als je meer nodig hebt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sklearn.datasets \n",
    "\n",
    "iris_features, iris_species = sklearn.datasets.load_iris(return_X_y=True, as_frame=True)\n",
    "\n",
    "np.set_printoptions(suppress=True)  # Wetenschappelijke notatie uitzetten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a47b6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dd9d2f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Bij clustering ga je op zoek naar logische groepen in je data. In 2 dimensies is dit makkelijk te visualiseren:\n",
    "\n",
    "![clustering](../afbeeldingen/MachineLearning/kmeans.png \"Finding clusters with k-means\")\n",
    "\n",
    "Bron: https://pranshu453.medium.com/k-means-clustering-simplified-in-python-ab57b8d629db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963c34d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Je ziet hier dat datapunten die dicht bij elkaar liggen samen in één cluster worden gestopt. Als deze clusters een betekenis hebben, bijvoorbeeld: 'jonge klanten die van actiefilms houden'. Of 'vrouwelijke klanten die van thrillers houden', dan kun je die informatie gebruiken om bijvoorbeeld gerichte marketingcampagnes te maken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9f358",
   "metadata": {},
   "source": [
    "Bij _k_-means clustering kies je zelf het aantal clusters dat je wilt maken (_k_) en het algoritme gaat dan op zoek naar de clusters. Het k-means algoritme is heel simpel:\n",
    "1. Kies k willekeurige datapunten als middelpunt (*centroid*) van een cluster\n",
    "2. Wijs ieder datapunt toe aan het cluster waarvan het middelpunt het dichtstbij is. \n",
    "3. Bepaal voor elk van de in stap 2 gevonden clusters het gemiddelde van de datapunten. Dit is het nieuwe middelpunt van het cluster.\n",
    "4. Herhaal stap 2 en 3 totdat de centra niet meer veranderen of totdat een maximum aantal iteraties is bereikt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb241dc",
   "metadata": {},
   "source": [
    "We gaan dit illustreren op gegenereerde data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef90599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genereer_data(num_samples=500, num_clusters_true=5, random_seed=42):\n",
    "    \"\"\"Genereert synthetische data voor K-Means.\"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    X = []\n",
    "    # Genereer data rondom verschillende 'echte' centra\n",
    "    centers = np.array([[-3, 3], [-3,2], [0,1], [0, -2], [4, 4]])\n",
    "    for i in range(num_clusters_true):\n",
    "        X.append(np.random.randn(num_samples // num_clusters_true, 2) * 0.8 + centers[i])\n",
    "    X = np.vstack(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf101bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_data = genereer_data()\n",
    "plt.scatter(x = kmeans_data[:,0], y = kmeans_data[:,1])\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e60a44d",
   "metadata": {},
   "source": [
    "In deze dataset zie je met het oog al dat er waarschijnlijk 4 (misschien 3?) logische clusters te maken zijn. Hierboven hadden we de stappen van k-means al beschreven en in het college zullen we hier nog even op ingaan. Hieronder gaan we kijken hoe we sklearn kunnen gebruiken om een k-means model te trainen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba8262",
   "metadata": {},
   "source": [
    "Dit gaat als volgt:\n",
    "1. Maak een k-Means model aan (zie https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "2. Definieer de dataset (we hoeven hier niet te splitsen in train en test, want we gaan geen voorspelling doen, we gaan alleen clusters maken)\n",
    "3. Kies de hyperparameters, zoals het aantal clusters (k)\n",
    "4. Train het model (door `fit()` aan te roepen)\n",
    "5. Toon de resultaten (zoals de labels en de centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc09cc5",
   "metadata": {},
   "source": [
    "Een voorbeeld op deze gegenereerde dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af3163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 1: maak kMeans model aan\n",
    "kmeans_model = KMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd4d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 2: definieer dataset (in dit geval hebben we die al, namelijk kmeans_data)\n",
    "X = kmeans_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff11b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 3: Kies de hyperparameters (wij kiezen nu alleen het aantal clusters (4))\n",
    "kmeans_model.set_params(n_clusters = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d2949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 4: train het model\n",
    "kmeans_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7c623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 5: bekijk resultaten:\n",
    "print(f\"De gevonden cluster middelpunten:\\n {kmeans_model.cluster_centers_}\")\n",
    "print(f\"\\nDe labels van de datapunten:\\n {kmeans_model.labels_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e678cdb",
   "metadata": {},
   "source": [
    "Deze kunnen we nu ook visualiseren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62bd219",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = kmeans_data[:,0], y = kmeans_data[:,1], c = kmeans_model.labels_, cmap='viridis')\n",
    "plt.scatter(x = kmeans_model.cluster_centers_[:,0], y = kmeans_model.cluster_centers_[:,1], s=100, c='red', marker='x', label='Centroids')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('K-Means Clustering Resultaat')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b4bb3",
   "metadata": {},
   "source": [
    "Stel nu dat dat we en nieuw datapunt hebben, bijvoorbeeld het punt $(-2, 1.5)$. Dan kunnen we het kMeans model gebruiken om te bepalen bij welk cluster deze hoort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [-2, 1.5]\n",
    "kmeans_model.predict([new_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b96561e",
   "metadata": {},
   "source": [
    "Dit laat zien dat het bij cluster met het nummer 0 hoort (in het plaatje het donker paarse cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20be9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = kmeans_data[:,0], y = kmeans_data[:,1], c = kmeans_model.labels_, cmap='viridis')\n",
    "plt.scatter(new_data[0], new_data[1], c='red', label='Nieuw datapunt')\n",
    "plt.legend()\n",
    "plt.title('K-Means Clustering Resultaat')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02bd350",
   "metadata": {},
   "source": [
    "### Visualisatie van k-means stappen\n",
    "Om een goed beeld te krijgen van hoe de stappen van k-means eruit zien, is deze site een aanrader: https://www.naftaliharris.com/blog/visualizing-k-means-clustering/\n",
    "\n",
    "Je kunt hier voor verschillende datasets en verschillende clusteraantallen zien hoe de clusters worden gevormd. Je kunt het aantal clusters kiezen (door herhaaldelijk op add centroids) en dan door op 'Update Centroids' en 'Reassign Points' te klikken zie je de stappen van k-means. Je ziet hier ook de zogenoemde Voronoi diagrammen onstaan, die de gebieden weergeven die bij een cluster horen.\n",
    "\n",
    "Begin bij de dataset Guassian Mixture en Packed Circles. Je kunt bij deze datasets visueel goed zijn hoeveel clusters er ongeveer zijn. Kies eerst _minder_ centroids dan er clusters zijn en blijf updaten tot het model niet meer verandert. Voeg dan nog wat centroids toe zodat het aantal overeenkomt met het aantal clusters dat je ziet. Update opnieuw tot het model is geconvergeerd. Voeg tenslotte nog enkele centroids toe, zodat het er waarschijnlijk _te veel_ zijn en kijk wat er gebeurt bij het herhaaldelijk updaten van het model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f695bb91",
   "metadata": {},
   "source": [
    "### ML Opdracht 4.1\n",
    "We bekijken een klein datasetje waarin voor 8 ijswinkels staat hoeveel chocolade- en hoeveel vanilleijs er verkocht is. Die data wordt in onderstaande cel ingeladen. Maak 2 clusters met behulp van sklearn en visualiseer de gevonden clusters. Kun je een logica vinden in de clusters? Is er bijvoorbeeld een cluster van winkels die veel chocola verkopen en een ander cluster van winkels die veel vanille verkopen? \n",
    "\n",
    "Bij welk cluster zou het een winkel met 10 verkochte chocoladeijsjes en 5 verkochte vanille-ijsjes horen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a2b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ijswinkel_verkoop = pd.DataFrame({'Chocolade': [12,15,18,10,8,9,12,20],'Vanille': [6,16,17,8,7,6,8,18]})\n",
    "ijswinkel_verkoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d1bc3",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "ijs_model = KMeans(n_clusters=2).fit(ijswinkel_verkoop)\n",
    "winkel = [10, 5]\n",
    "\n",
    "plt.scatter(x=ijswinkel_verkoop['Chocolade'], y=ijswinkel_verkoop['Vanille'], c=ijs_model.labels_)\n",
    "plt.scatter(x=ijs_model.cluster_centers_[:,0], y=ijs_model.cluster_centers_[:,1], s=100, c='red', marker='x', label='Centroids')\n",
    "plt.scatter(winkel[0], winkel[1], c='red', label='Nieuw datapunt')\n",
    "plt.legend()\n",
    "plt.title('IJswinkels')\n",
    "plt.xlabel(ijswinkel_verkoop.columns[0])\n",
    "plt.ylabel(ijswinkel_verkoop.columns[1])\n",
    "plt.show()\n",
    "\n",
    "ijs_model.predict([winkel])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e33e4",
   "metadata": {},
   "source": [
    "### ML Opdracht 4.2\n",
    "Nu gaan we een clustering uitvoeren op een echte dataset. We nemen hiervoor de iris dataset (zie iris_features.csv), dit is een bekende dataset met data over verschillende soorten irissen (de bloemen). De dataset bevat de volgende features:\n",
    "- Sepal length (cm): de lengte van de kelkbladeren\n",
    "- Sepal width (cm): de breedte van de kelkbladeren\n",
    "- Petal length (cm): de lengte van de kroonbladeren\n",
    "- Petal width (cm): de breedte van de kroonbladeren\n",
    "\n",
    "De onderzoeker vermoed dat er verschillende soorten irissen zijn, aan jou de taak om logische clusters te vinden om deze soorten te onderscheiden.\n",
    "\n",
    "a. Kies 2 features uit en visualiseer de data. Bedenk wat een logisch aantal clusters is en voer de clustering uit.\n",
    "\n",
    "b. Voer nu een k-means clustering uit op de hele dataset (dus alle 4 de features). Visualiseer de gevonden clusters door naar verschillende combinaties van features te kijken. Hoeveel verschillende irissoorten zijn er volgens jou?\n",
    "\n",
    "c. Stel we meten een nieuwe bloem op, en die heeft de volgende eigenschappen:\n",
    "- Sepal length: 5.8\n",
    "\n",
    "- Sepal width: 3.7\n",
    "\n",
    "- Petal length: 1.2\n",
    "\n",
    "- Petal width: 0.2\n",
    "\n",
    "Bij welk cluster zou die horen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3489e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris_features)\n",
    "plt.show()\n",
    "\n",
    "iris_features.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model = KMeans(n_clusters=2).fit(iris_features)\n",
    "new_iris = [5.8, 3.7, 1.2, 0.2]\n",
    "\n",
    "plt.scatter(x=iris_features['petal length (cm)'], y=iris_features['sepal length (cm)'], s=15, c=iris_model.labels_)\n",
    "plt.scatter(x=iris_model.cluster_centers_[:, 2], y=iris_model.cluster_centers_[:, 0], s=50, c='red', marker='x')\n",
    "plt.scatter(new_iris[2], new_iris[0], c='red', s=15, label='Nieuw datapunt')\n",
    "plt.title('Iris dataset')\n",
    "plt.xlabel(iris_features.columns[2])\n",
    "plt.ylabel(iris_features.columns[0])\n",
    "plt.show()\n",
    "\n",
    "iris_model.predict([new_iris])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f24add",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_features_labeled = iris_features.copy()\n",
    "iris_features_labeled['cluster'] = iris_model.labels_\n",
    "\n",
    "sns.pairplot(iris_features_labeled, hue='cluster')\n",
    "plt.show()\n",
    "\n",
    "iris_features_labeled.sample(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030335bb",
   "metadata": {},
   "source": [
    "## Het bepalen van k\n",
    "De belangrijkste parameter bij k-means is natuurlijk $k$, het aantal clusters. Bij 2-dimensionale data kun je een scatterplot maken en op het oog het aantal clusters bepalen, maar je hebt net bij de Iris dataset al gezien dat dat veel lastiger wordt met meer dimensies. Om dan te bepalen welk aantal clusters het beste is, hebben we een maat nodig die de bepaald hoe goed de gevonden clusters zijn. \n",
    "Hiervoor gebruiken we de Within-Cluster Sum of Squares (WCSS), ook wel inertia genoemd. Hierbij bereken je voor ieder punt de afstand tot het dichtsbijzijnde clustercentrum, die afstanden worden gekwadrateerd en die tel je bij elkaar op. De formule hiervoor is:\n",
    "\n",
    "$$ WCSS = \\sum_{i=1}^{k} \\sum_{x_j \\in C_i} d(x_j,\\mu_i)^2 $$\n",
    "waarbij $C_i$ het cluster is, $x_j$ een datapunt in dat cluster en $\\mu_i$ het clustercentrum van dat cluster en $d(x_j, \\mu_i)$ is de afstand tussen het datapunt en het clustercentrum.\n",
    "Je hoeft dit niet zelf te kunnen uitrekenen, deze is beschikbaar in sklearn als de `inertia_` property van het kMeans model.\n",
    "\n",
    "Op iedere dataset zou je een intertia van 0 kunnen bereiken, namelijk door elk datapunt in een eigen cluster te stoppen. Maar dat heeft natuurlijk weinig zin. Het doel is dus niet om een zo klein mogelijke inertia te krijgen, maar om een goede balans te vinden tussen het aantal clusters en de intertia.\n",
    "\n",
    "Hiervoor kunnen we de *Elbow method* gebruiken. Daarbij plot je voor verschillende waarden van $k$ de inertia. Je zoekt dan naar een waarde van $k$ waarbij de inertia niet meer significant daalt, dit is het punt waar de 'elleboog' zit in de grafiek. Dit is het aantal clusters dat je zou kunnen kiezen. Het beste aantal clusters is altijd afhankelijk van wat je vervolgens met die geclusterde data wilt gaan doen, maar de elbow method geeft je een idee van wat een logisch aantal clusters zou kunnen zijn.\n",
    "\n",
    "Hieronder de code en visualisatie van hoe dat eruit zou zien: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf21c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#genereer data voor K-Means\n",
    "kmeans_data = genereer_data()\n",
    "\n",
    "max_k = 10\n",
    "inertia_values = []\n",
    "\n",
    "#train model voor verschillende waarden van k en bereken inertia\n",
    "for k in range(1, max_k + 1):\n",
    "    model_kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    model_kmeans.fit_predict(kmeans_data)\n",
    "    inertia_values.append([k, model_kmeans.inertia_])\n",
    "    \n",
    "#plot de inertia waarden tov k\n",
    "plt.plot([x[0] for x in inertia_values], [x[1] for x in inertia_values], marker='o')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42078f83",
   "metadata": {},
   "source": [
    "Je ziet hier dat de inertia waarde af blijven nemen als het aantal clusters toeneemt, maar op een gegeven moment is de afname nog maar heel klein. We gaan hier dan op zoek naar de knik, oftwel *elbow* in de grafiek. Hier zie je dat een toenamen van $k$ van 3 naar 4 nog een behoorlijke afname veroorzaakt, maar daarna niet meer. Het optimale aantal clusters is hier dus 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd955c7",
   "metadata": {},
   "source": [
    "### Opdracht 4.3\n",
    "Bepaal met de elbow methode hoeveel verschillende irissoorten (dus hoeveel verschillende clusters) er aanwezig zijn in de iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d567455",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "max_k = 10\n",
    "inertia_values = []\n",
    "\n",
    "for k in range(1, max_k + 1):\n",
    "    iris_model = KMeans(n_clusters=k)  # , random_state=42\n",
    "    iris_model.fit_predict(iris_features)\n",
    "    inertia_values.append([k, iris_model.inertia_])\n",
    "    \n",
    "plt.plot([x[0] for x in inertia_values], [x[1] for x in inertia_values], marker='o')\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.yticks([])\n",
    "plt.title(\"Iris dataset: inertia in $k$ clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb91b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model = KMeans(n_clusters=3, random_state=42)\n",
    "iris_model.fit_predict(iris_features)\n",
    "iris_features_labeled['cluster'] = iris_model.labels_\n",
    "\n",
    "sns.pairplot(iris_features_labeled, hue='cluster', palette='Set1')\n",
    "plt.show()\n",
    "\n",
    "iris_features_labeled.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9c353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nu de echte species\n",
    "\n",
    "iris_features_species = iris_features.copy()\n",
    "iris_features_species['cluster'] = iris_species\n",
    "\n",
    "sns.pairplot(iris_features_species, hue='cluster', palette='Set1')\n",
    "\n",
    "iris_features_species.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061da43",
   "metadata": {},
   "source": [
    "### Tekortkomingen van k-means\n",
    "Door de manier waarop k-means werkt, worden er cirkelvormige clusters gevormd. Dit kan handig zijn als je data ook circelvormige clusters bevat, maar dat is zeker niet altijd het geval. \n",
    "\n",
    "### Opdracht 4.3\n",
    "Ga weer naar https://www.naftaliharris.com/blog/visualizing-k-means-clustering/ en kies voor de dataset Smiley Face. Lukt het je om hier met k-means een goede clustering te maken?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71cc6dc",
   "metadata": {},
   "source": [
    "Uiteraard is het niet realistisch dat je data de vorm heeft van een smiley, maar onderstaande vormen kun je wel tegenkomen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f9ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(13)\n",
    "X_stretched = np.dot(kmeans_data, rng.randn(2, 2))\n",
    "plt.scatter(X_stretched[:, 0], X_stretched[:, 1], s=10)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b2a78f",
   "metadata": {},
   "source": [
    "Je ziet hier ellipsvormige clusters, als we hier k-means op toepassen dan zullen de clusters niet goed gevonden worden, zoals onderstaande visualisatie laat zien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91133222",
   "metadata": {},
   "outputs": [],
   "source": [
    "stretched_kmeans_model = KMeans(n_clusters=3, random_state=42)\n",
    "stretched_kmeans_model.fit(X_stretched)\n",
    "plt.scatter(X_stretched[:, 0], X_stretched[:, 1], c=stretched_kmeans_model.labels_, cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dee9ed7",
   "metadata": {},
   "source": [
    "Er zijn verschillende alternatieve clusteralgoritmes die hier beter werken. Een voorbeeld is Guassian Mixture Models (GMM), die is specifiek goed in het vinden van deze ellipsvormige clusters. Deze behandelen we niet in de les, maar je kunt hier meer informatie vinden: https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d42a18a",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "Een ander clusteralgoritme dat om kan gaan met clusters van verschillende vormen is _Density-Based Spatial Clustering of Applications with Noise_ (DBSCAN). Dit algoritme werkt niet met een vooraf bepaald aantal clusters, maar zoekt clusters op basis van de dichtheid van de datapunten. Hierbij zijn twee parameters belangrijk:\n",
    "- **Epsilon (eps)**: Dit is de maximale afstand tussen twee punten om ze nog als in hetzelfde cluster te beschouwen. \n",
    "- **MinPts**: Dit is het minimum aantal punten dat binnen de epsilon afstand van een punt moet liggen om het als een kernpunt te beschouwen.\n",
    "\n",
    "Er wordt een random datapunt gekozen en er wordt gezocht naar alle punten die binnen de epsilon afstand van dat punt liggen. Zijn dit er meer dan MinPts, dan vormen deze punten een cluster. Vervolgens bekijk je voor alle punten binnen het cluster opnieuw of er punten binnen de epsilon afstand van dat punt liggen. Ook deze worden aan het cluster toegevoegd en zo breidt het cluster zich uit. Dit wordt herhaald tot er geen nieuwe punten meer gevonden worden binnen de epsilon afstand.\n",
    "\n",
    "Daarna wordt een nieuw punt gekozen die nog niet bij een cluster hoort, en worden deze stappen herhaald. \n",
    "\n",
    "Ondertaande plaatje laat dit zien (Bron: https://www.geeksforgeeks.org/machine-learning/dbscan-clustering-in-ml-density-based-clustering/):\n",
    "\n",
    "<img src=\"../afbeeldingen/MachineLearning/dbscan.png\" alt=\"DBSCAN clustering\" title=\"DBSCAN clustering\" width=\"400\">\n",
    "\n",
    "\n",
    "Het CORE POINT is het eerst gekozen punt, en alle punten die in de cirkel met straal epsilon liggen worden vormen samen een cluster. Vanaf de BORDER POINTS ga je dan op zoek naar nieuwe punten die bij het cluster horen. \n",
    "\n",
    "Ook hier wordt dit proces weer goed gevisualiseerd: https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/ Kies hier eens voor de Smiley Face en je ziet dat DBSCAN hier wel een goede clustering kan maken.\n",
    "\n",
    "Voor meer uitleg over DBSCAN zie: https://youtu.be/_A9Tq6mGtLI\n",
    "Of lees: https://www.geeksforgeeks.org/machine-learning/dbscan-clustering-in-ml-density-based-clustering/\n",
    "\n",
    "\n",
    "Uiteraard bevat sklearn ook een implementatie van DBSCAN, zie https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb49bfc",
   "metadata": {},
   "source": [
    "### Opdracht 4.4\n",
    "1. Bekijk de DBSCAN documentatie van sklearn en pas dit toe op de `X_stretched` data. Visualiseer de gevonden clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c0b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "stretched_dbscan_model = DBSCAN(eps=0.5, min_samples=5)\n",
    "stretched_dbscan_model.fit(X_stretched)\n",
    "plt.scatter(X_stretched[:, 0], X_stretched[:, 1], c=stretched_dbscan_model.labels_)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "print(int((stretched_dbscan_model.labels_ == -1).sum()), 'outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra: de evaluatie met inertia werkt niet goed bij het DBSCAN-model, hiervoor\n",
    "# kan Density-Based Clustering Validation (DBCV) gebruikt worden. Deze methode is\n",
    "# (nog) niet beschikbaar in sklearn, dus als je dit wil proberen, kan je een\n",
    "# implementatie uit deze repository halen: https://github.com/christopherjenness/DBCV\n",
    "\n",
    "# from DBCV import DBCV\n",
    "# print(\"k-Means validity (DBCV):\", DBCV(X_stretched, stretched_kmeans_model.labels_))\n",
    "# print(\"DBSCAN validity (DBCV):\", DBCV(X_stretched, stretched_dbscan_model.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0bfc6c",
   "metadata": {},
   "source": [
    "2. Pas DBSCAN ook toe op de Iris dataset en visualiseer de clusters en de 'ruis'-datapunten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6adc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dbscan_model = DBSCAN(eps=.5, min_samples=5)\n",
    "iris_dbscan_model.fit_predict(iris_features)\n",
    "\n",
    "iris_features_clustered = iris_features.copy()\n",
    "iris_features_clustered['cluster'] = iris_dbscan_model.labels_\n",
    "\n",
    "sns.pairplot(iris_features_clustered, hue='cluster', palette='Set1')\n",
    "\n",
    "for c in range(-1, max(iris_dbscan_model.labels_)+1):\n",
    "    print(f\"Cluster {c}: {(iris_dbscan_model.labels_ == c).sum()} members\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b1d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features schalen!\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "iris_features_norm = pd.DataFrame(scaler.fit_transform(iris_features), index=iris_features.index, columns=iris_features.columns)\n",
    "\n",
    "print(iris_features_norm.head(3), end='\\n\\n')\n",
    "\n",
    "print(iris_features[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dbscan_model = DBSCAN(eps=1., min_samples=5)\n",
    "iris_dbscan_model.fit_predict(iris_features_norm)\n",
    "\n",
    "iris_features_norm_clustered = iris_features_norm.copy()\n",
    "iris_features_norm_clustered['cluster'] = iris_dbscan_model.labels_\n",
    "\n",
    "sns.pairplot(iris_features_norm_clustered, hue='cluster', palette='Set1')\n",
    "\n",
    "for c in range(-1, max(iris_dbscan_model.labels_)+1):\n",
    "    print(f\"Cluster {c}: {(iris_dbscan_model.labels_ == c).sum()} members\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf9727f",
   "metadata": {},
   "source": [
    "### Andere clustering technieken\n",
    "Naast DBSCAN en k-means zijn er nog veel meer technieken om data te clusteren. Dit plaatje geeft een mooie visuele weergave van verschillende technieken en de vormen clusters die ze kunnen vinden.\n",
    "![cluster comparison](../afbeeldingen/MachineLearning/cluster_comparison.png 'Vergelijking van verschillende cluster technieken')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ca3837",
   "metadata": {},
   "source": [
    "### ML Opdracht 4.5\n",
    "Nu gaan we clustering toepassen op een interessantere dataset, namelijk de Credit Card Customer Segmentation dataset (`CC_GENERAL.csv`). Deze dataset bevat informatie over klanten van een creditcardmaatschappij en hun transactiegedrag. We gaan deze data gebruiken om clusters te identificeren die ons inzicht geven in verschillende klantsegmenten.\n",
    "\n",
    "De dataset komt van kaggle (https://www.kaggle.com/datasets/arjunbhasin2013/ccdata/data) en hier vind je ook de beschrijving van de kolommen. Vooral de kolommen `BALANCE`, `PURCHASES`, `ONEOFF_PURCHASES`, en `PAYMENTS` zijn interessant voor clustering, omdat ze informatie geven over het transactiegedrag van de klanten. Kun je bijvoorbeeld klanten vinden met een laag saldo maar hoge uitgaven? Of klanten met een hoog saldo en weinig uitgaven? \n",
    "Probeer zelf ook nog andere inzichten te vinden in de data. Geven k-means en DBSCAN dezelfde clusters? \n",
    "\n",
    "**Tip**: In deze dataset zitten features met verschillende schalen. Omdat clustering met afstanden werkt, is het belangrijk dat je je data eerst normaliseert of standaardiseert. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e80ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credit = pd.read_csv('../databronnen/CC_GENERAL.csv')\n",
    "df_credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cbd29a",
   "metadata": {},
   "source": [
    "### ML Opdracht 4.6\n",
    "\n",
    "Het leuke van k-means is dat het een heel simpel algoritme is en dus ook vrij makkelijk is om zelf te implementeren. Doe een poging om (zonder copilot, want die kan dat wel) een eigen k-means te implementeren. Dit hoeft niet meteen een KMeans klasse te zijn waar je een `fit()` functie op aan kunt roepen (mag uiteraard wel), maar maak gewoon een functie die gegeven een dataset en een aantal clusters, de clusters voor je bepaalt. Probeer de resultaten ook te visualiseren. \n",
    "\n",
    "**Hint 1**: begin met het schrijven van een functie de afstanden tussen twee punten berekent. We hebben dit bij k-NN al even gezien, maar de formule is simpel, de euclidische afstand tussen twee punten $(x_1, y_1)$ en $(x_2, y_2)$ is: $\\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}$.\n",
    "\n",
    "**Hint 2**: schrijf een functie die voor ieder datapunt het cluster bepaalt (oftwel stap 2 hierboven) en schrijf een functie die de nieuwe centroids bepaalt (stap 3 hierboven). Schrijf vervolgens de logica om deze stappen te herhalen totdat de centroids niet meer veranderen (of een maximum aantal iteraties is bereikt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d87676",
   "metadata": {},
   "source": [
    "Bij _k_-means clustering kies je zelf het aantal clusters dat je wilt maken (_k_) en het algoritme gaat dan op zoek naar de clusters. Het k-means algoritme is heel simpel:\n",
    "1. Kies k willekeurige datapunten als middelpunt (*centroid*) van een cluster\n",
    "2. Wijs ieder datapunt toe aan het cluster waarvan het middelpunt het dichtstbij is. \n",
    "3. Bepaal voor elk van de in stap 2 gevonden clusters het gemiddelde van de datapunten. Dit is het nieuwe middelpunt van het cluster.\n",
    "4. Herhaal stap 2 en 3 totdat de centra niet meer veranderen of totdat een maximum aantal iteraties is bereikt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a70804a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-s3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
