{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a73e58e",
   "metadata": {},
   "source": [
    "# Simulatie - les 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c927d",
   "metadata": {},
   "source": [
    "Notebook bij les 3 van de leerlijn simulatie van S3 - AI. \n",
    "\n",
    "© Auteur: Joost Vanstreels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f65a5",
   "metadata": {},
   "source": [
    "**Voorbereiding voor les 3:**\n",
    "- We kijken weer even terug naar de uniforme, normale en lognormale verdeling die zijn besproken in het eerste college over Distributies (zie data_les5.ipynb). Kijk nog even terug naar dat college."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import benodigde libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import anderson_ksamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6cfbf1",
   "metadata": {},
   "source": [
    "## Synthetische data\n",
    "Tijdens colleges 1 en 2 hebben we gebruikt gemaakt van een bestaande dataset. In de praktijk wordt echter vaak gebruik gemaakt van *synthetische* data. Dat is data die gegenereerd is op basis van kenmerken van historische data. Er zijn meerdere redenen voor het gebruik van synthetische data:\n",
    "- Synthetische data kan onbeperkt gegenereerd worden, dat is handig wanneer de historische dataset klein is;\n",
    "- Soms is er geen historische data beschikbaar, maar zijn wel kenmerken van een event beschikbaar (zoals de centrum- en spreidingsmaten) waarmee data gegenereerd kan worden;\n",
    "- Historische data kan toevalligheden bevatten waarop een model of simulatie kan *overfitten*;\n",
    "- Met synthetische data kunnen nieuwe situaties nagebootst worden, denk bij de ICU aan minder of juist meer instroom. Tijdens de Covid-pandemie konden ICU's verschillende scenario's met instroom van patiënten genereren.\n",
    "\n",
    "Tijdens dit college gaan we kijken naar manieren om synthetische data te genereren om complexere experimenten uit te voeren. We gaan er vanuit dat de historische data een bepaalde verdeling volgt. We blikken daarom eerst terug op de verschillende verdelingen die behandeld zijn en introduceren een vierde verdeling: de Poisson-verdeling. Daarna bekijken we drie methodes om te bepalen welke verdeling van toepassing is en wat de parameters waren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45a3dae",
   "metadata": {},
   "source": [
    "## Data ICU casus\n",
    "Voordat we de theorie in duiken, is het goed om eerst te realiseren wat we later in dit college gaan doen met het genereren van synthetische data. Voor de ICU casus gebruikte we een historische dataset met daarin per tijdstip een bepaald aantal patiënten dat arriveerde bij de ICU en voor elke patiënt een bepaalde ligtijd in bed. We gaan straks proberen te achterhalen hoe deze aankomstdata en ligdata verdeeld is zodat we synthetische data kunnen genereren die vergelijkbaar is met de historische data.\n",
    "\n",
    "De historische data is te vinden in de map ```examples\\icu\\simulatiedata.csv```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85909065",
   "metadata": {},
   "source": [
    "## Verdelingen\n",
    "Om synthetische data te kunnen genereren, moeten we ten eerste weten welke verdeling de data moet volgen en wat de waarden voor de parameters van de verdeling moeten zijn. We hebben eerder al gekeken naar de uniforme, normale en lognormale verdeling en we introduceren twee nieuwe verdelingen: de Poisson verdeling en de exponentiële verdeling.\n",
    "\n",
    "### Uniforme, normale en lognormale verdeling\n",
    "De uniforme, normale en lognormale verdeling zijn besproken in het eerste college over Distributies (zie data_les5.ipynb). Deze verdelingen verschillen erg van elkaar zoals je hieronder kunt zien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eb6756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniforme data genereren\n",
    "# Parameters\n",
    "bereik = np.arange(0,10) \n",
    "aantal = 10000\n",
    "\n",
    "# Random trekkingen uit verdeling maken\n",
    "uniforme_data = np.random.choice(bereik, aantal)\n",
    "\n",
    "# Normaalverdeelde data genereren\n",
    "# Parameters\n",
    "mu, sigma = 5, 2\n",
    "aantal = 10000\n",
    "\n",
    "# Random trekkingen uit verdeling maken\n",
    "normale_data = np.random.normal(mu, sigma, aantal)\n",
    "\n",
    "# Lognormaalverdeelde data genereren\n",
    "# Parameters zijn de mean en standaarddeviatie van de onderliggende normaalverdelingen, vandaar de np.log\n",
    "mu, sigma = np.log(2), np.log(1.5)\n",
    "aantal = 10000\n",
    "\n",
    "# Random trekkingen uit verdeling maken\n",
    "lognormale_data = np.random.lognormal(mu, sigma, aantal)\n",
    "\n",
    "# Data plotten\n",
    "sns.histplot(data=uniforme_data, color=\"skyblue\", label=\"Uniforme data\", kde=False)\n",
    "sns.histplot(data=normale_data, color=\"red\", label=\"Normaalverdeelde data\", kde=False)\n",
    "sns.histplot(data=lognormale_data, color=\"green\", label=\"Lognormaalverdeelde data\", kde=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0655291a",
   "metadata": {},
   "source": [
    "### Poissonverdeling\n",
    "De normale verdeling is een continue verdeling, en die kun je gebruiken om de kans uit te rekenen dat een normaalverdeelde variabele een bepaalde waarde aanneemt. Denk aan het berekenen van de kans dat een pinguin een snavellengte heeft van meer dan 50mm.\n",
    "\n",
    "De Poisson-verdeling is een discrete verdeling en wordt gebruikt om het aantal gebeurtenissen in een vast tijdsinterval te modelleren, waarbij de gebeurtenissen onafhankelijk van elkaar plaatsvinden. Denk aan:\n",
    "- Het aantal klanten dat in een uur een winkel binnenkomt.\n",
    "- Het aantal telefoontjes dat een callcenter ontvangt in een bepaalde periode.\n",
    "- Het aantal defecte producten in een productielijn per uur.\n",
    "\n",
    "De Poisson-verdeling wordt gekarakteriseerd door één parameter, $\\lambda$ (lambda), die het gemiddelde aantal gebeurtenissen per tijdseenheid aangeeft. De kans op het waarnemen van $k$ gebeurtenissen in een interval wordt gegeven door de formule:\n",
    "$$ P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!} $$\n",
    "\n",
    "Een voorbeeld:\n",
    "Stel er komen gemiddeld 5 klanten per uur de winkel binnen. Wat is dan de kans dat in een gegeven uur precies 3 klanten binnenkomen? We willen berekenen $P(X = 3)$, waarbij $X$ het aantal klanten is dat binnenkomt in een uur. Nu kunnen we de formule invullen:\n",
    "$P(X = 3) = \\frac{5^3 e^{-5}}{3!} = 0.14$\n",
    "\n",
    "Of we rekenen het uit uitrekenen met python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definieer functie voor de poisson verdeling:\n",
    "def poisson(l, k):\n",
    "    return l**k * np.exp(-l) / math.factorial(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92771b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bereken P(X=3):\n",
    "print(f\"De kans dat er 3 mensen in een uur binnen komen is {poisson(5, 3):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da01544",
   "metadata": {},
   "source": [
    "Stel je nu voor dat de kassiere even stieken wilt gaan roken en ze wilt de kans uitrekenen dat er niemand binnenkomt in de tijd dat ze weg is. Hoe groot is de kans dat er in 10 minuten (de tijd die ze nodig heeft), niemand binnen komt?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1227ba",
   "metadata": {},
   "source": [
    "Om deze vraag te beantwoorden moeten we eerst de parameter $\\lambda$ omrekenen. We weten dat er gemiddeld 5 klanten per uur binnenkomen, dus in 10 minuten (1/6 uur) is dat $\\frac{5}{6}$. Hiermee rekenen we dan verder:\n",
    "\n",
    "$P(X = 0) = \\frac{(\\frac{5}{6})^0 e^{-\\frac{5}{6}}}{0!} = e^{-\\frac{5}{6}} = 0.43$\n",
    "\n",
    "Of met onze python funcie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5486864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"De kans dat er geen klanten binnenkomen als de kassiere weg is, is: {poisson(5/6, 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f2603c",
   "metadata": {},
   "source": [
    "### Opdracht 1\n",
    "Gemiddeld komen er per week 10 patienten met ziekte A naar het ziekenhuis. Bereken het volgende:\n",
    "1. De kans dat er op 1 dag 2 patienten met ziekte A binnenkomen.\n",
    "2. De kans dat er op 1 dag meer dan 2 patienten met ziekte A binnenkomen.\n",
    "\n",
    "**Tip 1:** Bedenk eerst wat de juiste waarde voor $\\lambda$ is. \n",
    "\n",
    "**Tip 2:** 'meer dan 2' patienten is niet makkelijk uitrekenen, het kan namelijk zijn dat er 3, 4, 5, etc patienten binnenkomen. Bereken hiervoor de kans op 0, 1 en 2 patienten en trek dit af van 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffe5005",
   "metadata": {},
   "source": [
    "### Kansdichtheidsgrafiek van de Poissonverdeling\n",
    "Om een beeld te krijgen van hoe de kansdichtheidsgrafiek van een Poisson-verdeling eruit ziet, gaan we 1000 uur simuleren waarin gemiddeld 5 klanten per uur binnenkomen. Hiervoor gebruiken we `np.random.poisson` (zie: https://numpy.org/doc/2.1/reference/random/generated/numpy.random.poisson.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce77d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stel de parameters in\n",
    "rate = 5  # gemiddelde aantal klanten per uur\n",
    "aantal = 1000  # aantal simulaties\n",
    "\n",
    "# Random trekkingen uit verdeling maken\n",
    "poisson_data = np.random.poisson(lam=rate, size=aantal)\n",
    "# Data plotten\n",
    "plt.hist(poisson_data, bins=(poisson_data.max()-poisson_data.min()), edgecolor='black')\n",
    "plt.title('Kansdichtheid Poisson-verdeling (λ = 5)')\n",
    "plt.xlabel('Aantal klanten')\n",
    "plt.ylabel('Frequentie')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22649cac",
   "metadata": {},
   "source": [
    "Je ziet in deze grafiek een piek bij 5 klanten, wat logisch is omdat dat het gemiddelde aantal per uur is. Hoe verder we van dit gemiddelde af gaan, hoe kleiner de kans wordt. Uiteraard kunnen er niet minder dan 0 klanten per uur binnen komen, maar gebeurt het wel een enkele keer dat er heel veel (13 of 14) klanten binnen komen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96db46",
   "metadata": {},
   "source": [
    "Hieronder nog een voorbeeld van een Poisson-verdeling met een hele hoge lambda. Merk op hoe dit bij een groot genoege sample erg op een normaalverdeling gaat lijken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data volgens Poisson verdeling genereren\n",
    "\n",
    "# Parameters\n",
    "rate = 200\n",
    "aantal = 10000\n",
    "\n",
    "#set seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Random trekkingen uit verdeling maken\n",
    "poisson_data = np.random.poisson(lam=rate, size=aantal)\n",
    "\n",
    "# Data plotten\n",
    "sns.histplot(poisson_data, bins=100, stat=\"probability\", kde=True)\n",
    "plt.title('Kansdichtheid Poisson-verdeling (λ = 200)')\n",
    "plt.xlabel('Aantal klanten per uur')\n",
    "plt.ylabel('Kansdichtheid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ef1cc",
   "metadata": {},
   "source": [
    "### Sampelen uit een verdeling\n",
    "Je kent nu 4 verschillende verdelingen, de uniforme, normale, lognormale en poisson verdeling. Om zelf data te generen die aan een van deze verdelingen voldoet, kun je gebruik maken de random module van numpy. Belangrijk is telkens dat je de juiste parameters gebruikt.\n",
    "\n",
    "**Voorbeeld:**\n",
    " \n",
    "Trek 10 samples uit een normale verdeling met gemiddelde 10 en standaarddeviatie 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f5075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_normaal = np.random.normal(loc=10, scale=2, size=10)\n",
    "print(f\"10 samples uit een normale verdeling met mu = 10 en sd = 2: {samples_normaal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b65ca4e",
   "metadata": {},
   "source": [
    "### Opdracht 2\n",
    "1. Trek 1000 samples uit een uniforme verdeling tussen 0 en 10.\n",
    "2. Trek 1000 samples uit een lognormale verdeling met $\\mu = 4$ en $\\sigma = 1.5$ (beide zijn de waardes voor de onderliggende normaalverdeling).\n",
    "3. Trek 1000 samples uit een Poisson-verdeling met $\\lambda = 2$.\n",
    "4. Maak een visualisatie waarin je de histogrammen van de verschillende verdelingen vergelijkt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe71f0",
   "metadata": {
    "tags": [
     "docent"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1070bd20",
   "metadata": {},
   "source": [
    "## Verdeling bepalen\n",
    "We weten nu welke verdelingen er zijn en kunnen data genereren volgens zo'n verdeling. Maar we moeten het ook andersom kunnen doen: wanneer we data krijgen moeten we kunnen achterhalen welke verdeling er gevolgd wordt. Daar zijn drie manieren voor.\n",
    "\n",
    "### Methode 1. Plotten\n",
    "De meest simpele manier, maar ook de meest foutgevoelige manier, is het plotten van de verdeling van de data en *met het oog* bepalen of de verdelingen hetzelfde zijn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaf3bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testdata inladen\n",
    "df = pd.read_csv('../databronnen/testdata.csv')\n",
    "testdata = df['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twee histogrammen plotten\n",
    "plt.hist(samples_uniform, bins=100, alpha=0.5, label='Uniforme data', color='blue', density=True)\n",
    "plt.hist(testdata, bins=100, alpha=0.5, label='Testdata', color='red', density=True)\n",
    "\n",
    "# Adding legend\n",
    "plt.legend()\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Vergelijking van twee verdelingen')\n",
    "plt.xlabel('Waarde')\n",
    "plt.ylabel('Frequentie')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab556a4",
   "metadata": {},
   "source": [
    "Deze methode is interessant om te bepalen welke verdelingen sowieso afvallen, maar je kunt nog geen conclusie trekken of een verdeling wel past"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e4659",
   "metadata": {},
   "source": [
    "### Opdracht 3\n",
    "Pas bovenstaande visualisatie aan en bepaal welke van de vier gegeven datasets (```uniforme_data```, ```normale_data```, ```lognormale_data``` en ```poisson_data```) het beste overeenkomt met de testdata (```testdata```). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d58b5f",
   "metadata": {},
   "source": [
    "### Methode 2. QQ plot\n",
    "Een betere, maar ook nog steeds niet foutloze, manier is het maken van een *Quantile-Quantile* plot. Het idee is dat je de samples uit beide datasets sorteert in oplopende volgorde en daarna een scatterplot genereert. Wanneer de scatterplot een rechte lijn volgt, zijn de verdelingen hetzelfde.\n",
    "\n",
    "In dit blog wordt de werking erg goed uitgelegd: https://blog.dailydoseofds.com/p/a-visual-and-intuitive-guide-to-qq.\n",
    "\n",
    "#### Voorbeeld\n",
    "Stel dat je de volgende twee datasets hebt:\n",
    "1. ```[0,1,3,4,5,6]```\n",
    "2. ```[1,2,3,4,4,6]```\n",
    "\n",
    "De scatterplot bestaat uit de volgende punten: ```(0,1), (1,2), (3,3), (4,4), (5,4), (6,6)```. Wanneer je dit plot en een rechte lijn er doorheen trekt, kun je zien waar de afwijkingen liggen. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7497d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "dataset1 = np.array([0,1,3,4,5,6])\n",
    "dataset2 = np.array([1,2,3,4,4,6])\n",
    "\n",
    "# Data plotten\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(dataset1, dataset2, alpha=0.7, color='blue')\n",
    "\n",
    "# Rechte lijn plotten\n",
    "min_val = min(dataset1.min(), dataset2.min())\n",
    "max_val = max(dataset1.max(), dataset2.max())\n",
    "ax.plot([min_val, max_val], [min_val, max_val], 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5461118a",
   "metadata": {},
   "source": [
    "### Datasets met verschillende grootte\n",
    "Het voorbeeld hierboven gaat over twee datasets die even groot zijn, maar dat is geen vereiste. De QQ-plot, de naam zegt het al, gebruikt namelijk de data op bepaalde kwantielen om zo ook datasets van verschillende groottes te kunnen vergelijken.\n",
    "\n",
    "In de code hieronder zie je dat de eerste dataset twee keer zo groot is als de tweede. Er worden daarna 12 kwantielen gemaakt. ```x_quantiles``` is logischerwijs hetzelfde als ```dataset1``` maar ```y_quantiles``` is groter dan ```dataset2```. Het resultaat zijn twee gelijke datasets die vergeleken kunnen worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdedae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "dataset1 = np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "dataset2 = np.array([1,3,5,8,10,12])\n",
    "\n",
    "# Kwantieldata genereren\n",
    "x_quantiles = np.quantile(dataset1, np.linspace(0, 1, 12))\n",
    "y_quantiles = np.quantile(dataset2, np.linspace(0, 1, 12))\n",
    "\n",
    "# Data plotten\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(x_quantiles, y_quantiles, alpha=0.7, color='blue')\n",
    "\n",
    "# Rechte lijn plotten\n",
    "min_val = min(dataset1.min(), dataset2.min())\n",
    "max_val = max(dataset1.max(), dataset2.max())\n",
    "ax.plot([min_val, max_val], [min_val, max_val], 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e8c01",
   "metadata": {},
   "source": [
    "Het voordeel van deze QQ-plot ten opzichte van de verdelingen plotten, is dat je beter kunt bepalen of datasets écht dezelfde verdeling hebben of écht niet, maar bij twijfel geeft de QQ-plot nog steeds geen antwoord."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3582bec6",
   "metadata": {},
   "source": [
    "### Opdracht 4\n",
    "Maak voor elk van de vier datasets een QQ-plot met de testdata en bepaal welke van de vier datasets het beste overeenkomt met de testdata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3c07f3",
   "metadata": {},
   "source": [
    "### Methode 3. Anderson-Darling K-Sample Test\n",
    "De Anderson-Darling K-Sample test is een wiskundige methode om te bepalen of meerdere datasets (*k* is het aantal datasets) dezelfde verdeling volgen. Het resultaat van de formule is een waarde die de *p-waarde* wordt genoemd. Wanneer deze p-waarde lager dan 0.05 is, is aangetoond dat de datasets dezelfde verdeling volgen. \n",
    "\n",
    "#### Theorie achter de test\n",
    "Het idee achter de test is dat wanneer twee datasets dezelfde verdeling volgen, de cumulatieve frequentieverdelingen van beide datasets hetzelfde zijn. De Anderson-Darling test kwantificeert de verschillen tussen de cumulatieve frequentieverdelingen en kan daarmee een uitspraak doen. \n",
    "\n",
    "Hoe het proces precies verloopt is best complex, net zoals de formule:\n",
    "\n",
    "$$A_{k}^2 = \\frac{1}{N} \\sum_{j=1}^{n-1} \\frac{(NH_j - \\sum_{i=1}^k n_i F_{ij})^2}{H_j(1-H_j)}$$\n",
    "\n",
    "Hier zullen we niet verder op in gaan.\n",
    "\n",
    "#### Interpretatie resultaat\n",
    "Het belangrijkste is het resultaat van de formule: de p-waarde. Zo'n p-waarde wordt gebruikt bij het stellen van *hypotheses*. Vaak is de *nulhypothese* **negatief** geformuleerd. In dit geval is de nulhypothese: *\"De datasets volgen een andere verdeling.\"*\n",
    "\n",
    "Wanneer een p-waarde laag is (vaak < 0.05) wordt de nulhypothese *verworpen* (en is dus *niet* waar). Dat betekent in dit geval dat de datasets **niet** een andere verdeling volgen: ze volgen dus **wel** **dezelfde** verdeling! Wanneer de p-waarde >= 0.05 is, volgen ze niet dezelfde verdeling.\n",
    "\n",
    "#### Voordelen van de test\n",
    "Het resultaat is, in tegenstelling tot het plotten van de datasets en de QQ-plot, kwantificeerbaar en dus objectief. Er zijn ook andere tests die een kwantificeerbaar resultaat opleveren, maar die hebben nadelen zoals dat het al duidelijk moet zijn welke verdeling een dataset volgt of dat ze last hebben van grote datasets of outliers. De Anderson-Darling K-sample test heeft weinig nadelen, behalve dat deze niet zo goed werkt bij te kleine datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functie om de Anderson-Darling K-Sample Test van scipy aan te roepen\n",
    "def compare_distributions(data1, data2, alpha=0.05): \n",
    "    try:\n",
    "        ad_result = anderson_ksamp([data1, data2])\n",
    "        results = {\n",
    "            'statistic': ad_result.statistic,\n",
    "            'critical_values': ad_result.critical_values,\n",
    "            'significance_level': ad_result.significance_level,\n",
    "            'same_distribution': ad_result.significance_level > alpha\n",
    "        }\n",
    "    except Exception as e:\n",
    "        results = {\n",
    "            'statistic': None,\n",
    "            'same_distribution': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107a607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare_distributions(uniforme_data, testdata)\n",
    "\n",
    "for key, value in results.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbcc37d",
   "metadata": {},
   "source": [
    "In bovenstaande code zie je dat de *significance level* berekend wordt. Wanneer deze score groter is dan 0.05 wordt de hypothese verworpen en hebben de datasets dezelfde verdeling. \n",
    "\n",
    "Het voordeel van deze test is dat deze objectief is: er is een kwantitatieve score in tegenstelling tot de eerste twee methodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9362fadd",
   "metadata": {},
   "source": [
    "### Opdracht 5\n",
    "Bepaal voor elke van de vier gegeven datasets de significance level met de testdata. \n",
    "\n",
    "Bepaal op basis van de vorige opdrachten wat de verdeling is van de testdata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3194f",
   "metadata": {},
   "source": [
    "## Synthetische data genereren\n",
    "We hebben geleerd hoe je twee datasets kunt vergelijken. Bij de ICU casus hebben we historische data maar nog geen synthetische data: die moeten we eerst genereren. En daar lopen we tegen een *kip-ei probleem* aan:\n",
    "- Om synthetische data te kunnen genereren, moeten we weten welke verdeling de historische data volgt\n",
    "- Om te bepalen welke verdeling de historische data volgt, hebben we (synthetisch gegenereerde) testdata nodig\n",
    "\n",
    "We ontkomen er niet aan om voor de historische data set meerdere testdatasets te genereren: voor elke relevante verdeling een. We moeten eerst bepalen welke verdelingen relevant zijn: een Poissonverdeling is wel relevant voor de aankomstdata maar niet voor de ligdata bijvoorbeeld.\n",
    "\n",
    "Daarna kunnen we de historische data vergelijken met deze testdatasets en bepalen welke verdeling van toepassing is. We hanteren het volgende stappenplan:\n",
    "1. Bepaal welke verdelingen relevant zijn;\n",
    "2. Bepaal voor elke verdeling welke centrummaten, spreidingsmaten of andere parameters nodig zijn;\n",
    "3. Bereken deze parameters voor de historische dataset;\n",
    "4. Sample synthetische data op basis van deze parameters;\n",
    "5. Vergelijk de historische dataset met de synthetische dataset.\n",
    "\n",
    "Om data die een normaalverdeling volgt te kunnen genereren, moeten we dus het gemiddelde en de standaarddeviatie van de historische dataset berekenen. Daarna kunnen we een dataset genereren met een vergelijkbare grootte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1005f33",
   "metadata": {},
   "source": [
    "## Opdrachten ICU\n",
    "We gaan synthetische data genereren op basis van de historische dataset. De dataset bevat twee features waarvoor synthetische data gegenereerd moet worden: de aankomstdata en de ligdata. \n",
    "\n",
    "### Deel 1 - Aankomstdata\n",
    "\n",
    "**Opdracht 6.1.** Aantal aankomsten per tijdstip\n",
    "\n",
    "Voor elk tijdstip van de ICU, moeten we bepalen hoeveel patiënten arriveerden op elk tijdstip. Neem de historische dataset en bereken voor elk tijdstip hoeveel patiënten arriveerden. \n",
    "\n",
    "**Opdracht 6.2.** Genereer synthetische data en bepaal verdeling\n",
    "\n",
    "Genereer synthetische data volgens bovenstaand stappenplan en vergelijk deze data met de historische data. Kijk daarna welke van de relevante verdelingen het beste past. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc819f",
   "metadata": {},
   "source": [
    "### Deel 2 - Ligdata\n",
    "\n",
    "**Opdracht 7.** Genereer synthetische data\n",
    "\n",
    "De ligtijd is wel vastgelegd in de dataset. Genereer synthetische data en vergelijk deze data met de historische data. Kijk daarna welke van de vier genoemde verdelingen het beste past. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fdd590",
   "metadata": {},
   "source": [
    "### Deel 3 - Extra data manipulatie stap\n",
    "***Spoiler alert***: het is niet mogelijk om passende verdelingen te vinden voor de gegeven dataset. Echter, door de data eerst te manipuleren kan het wel...! Deze opdracht maakt onderdeel uit van het portfolioitem 'Simulatie'.\n",
    "\n",
    "**Opdracht 8.1.** Manipuleren\n",
    "\n",
    "Denk na over manieren waarop je de data kunt manipuleren (bijv. opschonen, groeperen, filteren, verwijderen, ...) en voer deze uit. Voor daarna opdrachten 8.2 en 8.3 uit tot je een passende verdeling vindt. Voer daarna pas opdracht 8.4 uit.\n",
    "\n",
    "**Opdracht 8.2.** Aantal aankomsten per tijdstip\n",
    "\n",
    "Bereken opnieuw het aantal aankomsten per tijdstip. \n",
    "\n",
    "**Opdracht 8.3.** Genereer synthetische data aankomstdata \n",
    "\n",
    "Genereer synthetische data en vergelijk deze data met de historische data. Kijk daarna welke van de vier genoemde verdelingen het beste past. \n",
    "\n",
    "**Opdracht 8.4.** Genereer synthetische data ligdata\n",
    "\n",
    "Genereer synthetische data en vergelijk deze data met de historische data. Kijk daarna welke van de vier genoemde verdelingen het beste past. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s3venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
